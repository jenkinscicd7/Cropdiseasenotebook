{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbd44ac-51b5-4d87-9666-e8b76fdf714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import PIL.Image as Image\n",
    "import os,glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling2D, GlobalMaxPooling2D,\n",
    "    Reshape, Dense, Conv2D, Multiply, Add, Activation,\n",
    "    Concatenate, BatchNormalization, ReLU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5e0935-0154-4efb-965b-20094d201f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('plant-village')\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a64b9cd-6916-4010-b114-f9b0debf2149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting image paths and labels...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9f3cf9cadd4787abcf6e94a40930c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classes: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10835 images across 9 classes.\n"
     ]
    }
   ],
   "source": [
    "all_image_paths = []\n",
    "all_image_labels = []\n",
    "\n",
    "print(\"Collecting image paths and labels...\")\n",
    "for class_dir in tqdm(base_dir.iterdir(), desc=\"Classes\"):\n",
    "    if class_dir.is_dir():\n",
    "        label = class_dir.name\n",
    "        for img_path in class_dir.glob(\"*.jpg\"):\n",
    "            all_image_paths.append(str(img_path))\n",
    "            all_image_labels.append(label)\n",
    "\n",
    "print(f\"Found {len(all_image_paths)} images across {len(set(all_image_labels))} classes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b38e2d0e-96aa-4ffd-8f55-aa12142e9b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label order used by model:\n",
      "0: Pepper__bell___Bacterial_spot\n",
      "1: Pepper__bell___healthy\n",
      "2: Potato___Early_blight\n",
      "3: Potato___Late_blight\n",
      "4: Potato___healthy\n",
      "5: Tomato_Leaf_Mold\n",
      "6: Tomato_Spider_mites_Two_spotted_spider_mite\n",
      "7: Tomato__Tomato_YellowLeaf__Curl_Virus\n",
      "8: Tomato__Tomato_mosaic_virus\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "all_image_labels_encoded = le.fit_transform(all_image_labels)\n",
    "\n",
    "print(\"Label order used by model:\")\n",
    "for idx, label in enumerate(le.classes_):\n",
    "    print(f\"{idx}: {label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96201db8-3df2-408a-be44-bce5f1e876c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 7259\n",
      "Test samples: 3576\n"
     ]
    }
   ],
   "source": [
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    all_image_paths, all_image_labels_encoded, test_size=0.33, random_state=0, stratify=all_image_labels_encoded\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_paths)}\")\n",
    "print(f\"Test samples: {len(test_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9a45272-069e-4575-8c23-0f4f37fa6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3, dct_method='INTEGER_ACCURATE')\n",
    "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.BILINEAR)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image.set_shape([IMG_HEIGHT, IMG_WIDTH, 3])  \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66c5b0b5-5058-44c8-ba13-c39075c18310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90afc9bd-e189-415f-adc0-f478b28ba5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_ds = train_ds.shuffle(len(train_paths))\n",
    "train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.cache(r\"D:\\Desktop\\tf_cache/train_cache.tf-data\").repeat()   \n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "test_ds = test_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d160c48-cb17-4780-b6b2-ba5406e74f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b23cd5bf-3436-477a-bdd7-edc947324abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(input_tensor, ratio=8):\n",
    "    \"\"\"Squeeze-and-Excitation block.\"\"\"\n",
    "    channel = input_tensor.shape[-1]\n",
    "    se = GlobalAveragePooling2D()(input_tensor)\n",
    "    se = Reshape((1, 1, channel))(se)\n",
    "    se = Dense(channel // ratio, activation='relu', use_bias=False)(se)\n",
    "    se = Dense(channel, activation='sigmoid', use_bias=False)(se)\n",
    "    return Multiply()([input_tensor, se])  # Squeeze and excite output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4399813-ddb5-415f-a364-6656cd7a90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbam_block_with_outputs(input_feature, ratio=8):\n",
    "    channel = input_feature.shape[-1]\n",
    "\n",
    "    \n",
    "    se_refined = se_block(input_feature, ratio=ratio)\n",
    "\n",
    "    # channel attention\n",
    "    shared_dense_one = Dense(channel // ratio, activation='relu', name='cbam_dense_1')\n",
    "    shared_dense_two = Dense(channel, name='cbam_dense_2')\n",
    "\n",
    "    avg_pool = GlobalAveragePooling2D()(se_refined)\n",
    "    avg_pool = Reshape((1, 1, channel))(avg_pool)\n",
    "    avg_out = shared_dense_two(shared_dense_one(avg_pool))\n",
    "\n",
    "    max_pool = GlobalMaxPooling2D()(se_refined)\n",
    "    max_pool = Reshape((1, 1, channel))(max_pool)\n",
    "    max_out = shared_dense_two(shared_dense_one(max_pool))\n",
    "\n",
    "    channel_att = Activation('sigmoid')(Add()([avg_out, max_out]))\n",
    "    channel_refined = Multiply()([se_refined, channel_att])  # (H, W, C)\n",
    "\n",
    "    # Spatial attention\n",
    "    avg_sp = tf.reduce_mean(channel_refined, axis=-1, keepdims=True)\n",
    "    max_sp = tf.reduce_max(channel_refined, axis=-1, keepdims=True)\n",
    "    concat = Concatenate()([avg_sp, max_sp])\n",
    "    spatial_att = Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n",
    "\n",
    "    spatial_refined = Multiply()([channel_refined, spatial_att])  # (H, W, C)\n",
    "\n",
    "    return spatial_refined, channel_att, spatial_att\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e860d40f-1afe-4727-a18f-5172257047a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cbam_model(num_classes):\n",
    "    input_tensor = tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "    # Use MobileNetV2 base\n",
    "    x = base_model(input_tensor, training=False)  # Shape: (7, 7, 1280)\n",
    "\n",
    "    # Apply CBAM\n",
    "    channel = x.shape[-1]\n",
    "    cbam_out, channel_att_map, spatial_att_map = cbam_block_with_outputs(x)\n",
    "\n",
    "    # Pool + classify\n",
    "    x = GlobalAveragePooling2D()(cbam_out)  # shape: (None, 1280)\n",
    "    predictions = Dense(num_classes, activation='softmax', name='classification')(x)\n",
    "\n",
    "    # Flatten channel attention for output\n",
    "    # This output is likely for visualization/analysis, not directly for loss calculation\n",
    "    channel_vector = Reshape((channel,))(channel_att_map)  # (None, 1280)\n",
    "    spatial_map = spatial_att_map  # (None, 7, 7, 1)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_tensor, outputs=[predictions, channel_vector, spatial_map])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70ebab08-e725-4684-97e4-22e6e7076b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Epoch 1/30\n",
      "226/226 [==============================] - 487s 2s/step - loss: 0.3771 - classification_loss: 0.3771 - classification_accuracy: 0.8844 - val_loss: 0.1514 - val_classification_loss: 0.1514 - val_classification_accuracy: 0.9488\n",
      "Epoch 2/30\n",
      "226/226 [==============================] - 392s 2s/step - loss: 0.0965 - classification_loss: 0.0965 - classification_accuracy: 0.9707 - val_loss: 0.1103 - val_classification_loss: 0.1103 - val_classification_accuracy: 0.9606\n",
      "Epoch 3/30\n",
      "226/226 [==============================] - 383s 2s/step - loss: 0.0579 - classification_loss: 0.0579 - classification_accuracy: 0.9812 - val_loss: 0.0725 - val_classification_loss: 0.0725 - val_classification_accuracy: 0.9749\n",
      "Epoch 4/30\n",
      "226/226 [==============================] - 382s 2s/step - loss: 0.0359 - classification_loss: 0.0359 - classification_accuracy: 0.9896 - val_loss: 0.0770 - val_classification_loss: 0.0770 - val_classification_accuracy: 0.9704\n",
      "Epoch 5/30\n",
      "226/226 [==============================] - 378s 2s/step - loss: 0.0297 - classification_loss: 0.0297 - classification_accuracy: 0.9903 - val_loss: 0.1345 - val_classification_loss: 0.1345 - val_classification_accuracy: 0.9572\n",
      "Epoch 6/30\n",
      "226/226 [==============================] - 380s 2s/step - loss: 0.0119 - classification_loss: 0.0119 - classification_accuracy: 0.9981 - val_loss: 0.1403 - val_classification_loss: 0.1403 - val_classification_accuracy: 0.9611\n",
      "Epoch 7/30\n",
      "226/226 [==============================] - 380s 2s/step - loss: 0.0079 - classification_loss: 0.0079 - classification_accuracy: 0.9978 - val_loss: 0.0699 - val_classification_loss: 0.0699 - val_classification_accuracy: 0.9797\n",
      "Epoch 8/30\n",
      "226/226 [==============================] - 376s 2s/step - loss: 0.0146 - classification_loss: 0.0146 - classification_accuracy: 0.9964 - val_loss: 0.0800 - val_classification_loss: 0.0800 - val_classification_accuracy: 0.9761\n",
      "Epoch 9/30\n",
      "226/226 [==============================] - 378s 2s/step - loss: 0.0112 - classification_loss: 0.0112 - classification_accuracy: 0.9964 - val_loss: 0.0927 - val_classification_loss: 0.0927 - val_classification_accuracy: 0.9730\n",
      "Epoch 10/30\n",
      "226/226 [==============================] - 380s 2s/step - loss: 0.0176 - classification_loss: 0.0176 - classification_accuracy: 0.9953 - val_loss: 0.0812 - val_classification_loss: 0.0812 - val_classification_accuracy: 0.9738\n",
      "Epoch 11/30\n",
      "226/226 [==============================] - 380s 2s/step - loss: 0.0100 - classification_loss: 0.0100 - classification_accuracy: 0.9976 - val_loss: 0.1378 - val_classification_loss: 0.1378 - val_classification_accuracy: 0.9581\n",
      "Epoch 12/30\n",
      "226/226 [==============================] - 383s 2s/step - loss: 0.0079 - classification_loss: 0.0079 - classification_accuracy: 0.9982 - val_loss: 0.0873 - val_classification_loss: 0.0873 - val_classification_accuracy: 0.9730\n",
      "Epoch 13/30\n",
      "226/226 [==============================] - 380s 2s/step - loss: 0.0013 - classification_loss: 0.0013 - classification_accuracy: 0.9999 - val_loss: 0.0641 - val_classification_loss: 0.0641 - val_classification_accuracy: 0.9797\n",
      "Epoch 14/30\n",
      "226/226 [==============================] - 433s 2s/step - loss: 4.3142e-04 - classification_loss: 4.3142e-04 - classification_accuracy: 1.0000 - val_loss: 0.0713 - val_classification_loss: 0.0713 - val_classification_accuracy: 0.9789\n",
      "Epoch 15/30\n",
      "226/226 [==============================] - 437s 2s/step - loss: 2.3162e-04 - classification_loss: 2.3162e-04 - classification_accuracy: 1.0000 - val_loss: 0.0721 - val_classification_loss: 0.0721 - val_classification_accuracy: 0.9792\n",
      "Epoch 16/30\n",
      "226/226 [==============================] - 393s 2s/step - loss: 1.8206e-04 - classification_loss: 1.8206e-04 - classification_accuracy: 1.0000 - val_loss: 0.0732 - val_classification_loss: 0.0732 - val_classification_accuracy: 0.9789\n",
      "Epoch 17/30\n",
      "226/226 [==============================] - 387s 2s/step - loss: 1.4732e-04 - classification_loss: 1.4732e-04 - classification_accuracy: 1.0000 - val_loss: 0.0745 - val_classification_loss: 0.0745 - val_classification_accuracy: 0.9789\n",
      "Epoch 18/30\n",
      "226/226 [==============================] - 380s 2s/step - loss: 1.2091e-04 - classification_loss: 1.2091e-04 - classification_accuracy: 1.0000 - val_loss: 0.0760 - val_classification_loss: 0.0760 - val_classification_accuracy: 0.9789\n",
      "Epoch 19/30\n",
      "226/226 [==============================] - 379s 2s/step - loss: 9.9612e-05 - classification_loss: 9.9612e-05 - classification_accuracy: 1.0000 - val_loss: 0.0779 - val_classification_loss: 0.0779 - val_classification_accuracy: 0.9783\n",
      "Epoch 20/30\n",
      "226/226 [==============================] - 379s 2s/step - loss: 8.2407e-05 - classification_loss: 8.2407e-05 - classification_accuracy: 1.0000 - val_loss: 0.0791 - val_classification_loss: 0.0791 - val_classification_accuracy: 0.9786\n",
      "Epoch 21/30\n",
      "226/226 [==============================] - 381s 2s/step - loss: 6.8865e-05 - classification_loss: 6.8865e-05 - classification_accuracy: 1.0000 - val_loss: 0.0805 - val_classification_loss: 0.0805 - val_classification_accuracy: 0.9783\n",
      "Epoch 22/30\n",
      "226/226 [==============================] - 381s 2s/step - loss: 5.7636e-05 - classification_loss: 5.7636e-05 - classification_accuracy: 1.0000 - val_loss: 0.0819 - val_classification_loss: 0.0819 - val_classification_accuracy: 0.9783\n",
      "Epoch 23/30\n",
      "226/226 [==============================] - 383s 2s/step - loss: 4.8845e-05 - classification_loss: 4.8845e-05 - classification_accuracy: 1.0000 - val_loss: 0.0833 - val_classification_loss: 0.0833 - val_classification_accuracy: 0.9783\n",
      "Epoch 24/30\n",
      "226/226 [==============================] - 431s 2s/step - loss: 4.1604e-05 - classification_loss: 4.1604e-05 - classification_accuracy: 1.0000 - val_loss: 0.0847 - val_classification_loss: 0.0847 - val_classification_accuracy: 0.9783\n",
      "Epoch 25/30\n",
      "226/226 [==============================] - 445s 2s/step - loss: 3.5432e-05 - classification_loss: 3.5432e-05 - classification_accuracy: 1.0000 - val_loss: 0.0861 - val_classification_loss: 0.0861 - val_classification_accuracy: 0.9783\n",
      "Epoch 26/30\n",
      "226/226 [==============================] - 444s 2s/step - loss: 3.0649e-05 - classification_loss: 3.0649e-05 - classification_accuracy: 1.0000 - val_loss: 0.0875 - val_classification_loss: 0.0875 - val_classification_accuracy: 0.9783\n",
      "Epoch 27/30\n",
      "226/226 [==============================] - 442s 2s/step - loss: 2.6297e-05 - classification_loss: 2.6297e-05 - classification_accuracy: 1.0000 - val_loss: 0.0888 - val_classification_loss: 0.0888 - val_classification_accuracy: 0.9783\n",
      "Epoch 28/30\n",
      "226/226 [==============================] - 444s 2s/step - loss: 2.2924e-05 - classification_loss: 2.2924e-05 - classification_accuracy: 1.0000 - val_loss: 0.0902 - val_classification_loss: 0.0902 - val_classification_accuracy: 0.9786\n",
      "Epoch 29/30\n",
      "226/226 [==============================] - 459s 2s/step - loss: 2.0034e-05 - classification_loss: 2.0034e-05 - classification_accuracy: 1.0000 - val_loss: 0.0915 - val_classification_loss: 0.0915 - val_classification_accuracy: 0.9786\n",
      "Epoch 30/30\n",
      "226/226 [==============================] - 453s 2s/step - loss: 1.7394e-05 - classification_loss: 1.7394e-05 - classification_accuracy: 1.0000 - val_loss: 0.0929 - val_classification_loss: 0.0929 - val_classification_accuracy: 0.9789\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(le.classes_)\n",
    "model = build_cbam_model(num_classes=num_classes)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'classification':'sparse_categorical_crossentropy'},  \n",
    "    metrics={'classification':'accuracy'}\n",
    ")\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_ds,\n",
    "    steps_per_epoch=len(train_paths) // BATCH_SIZE,\n",
    "    validation_steps=len(test_paths) // BATCH_SIZE\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d3d8e30-3f2e-4cbc-a0d6-4aa1421b7db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_models/cbam_with_attention_outputs_se_tfdataiiD\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_models/cbam_with_attention_outputs_se_tfdataiiD\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kariu\\AppData\\Local\\Temp\\tmp93wheaz7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kariu\\AppData\\Local\\Temp\\tmp93wheaz7\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved and converted to TFLite.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"../saved_models/cbam_with_attention_outputs_se_tfdataiiD\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"rcbam_attention_tfdata.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model saved and converted to TFLite.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "929ff8f3-ea6a-4378-a579-c642b820fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on the test set...\n",
      "112/112 [==============================] - 151s 1s/step\n",
      "Confusion Matrix:\n",
      "[[ 323    4    0    0    0    0    1    1    0]\n",
      " [   5  483    0    0    0    0    0    0    0]\n",
      " [   0    0  323    7    0    0    0    0    0]\n",
      " [   0    0    6  316    7    0    1    0    0]\n",
      " [   0    0    0    0   50    0    0    0    0]\n",
      " [   0    0    0    0    0  306    2    3    3]\n",
      " [   0    2    0    0    0   11  530    6    4]\n",
      " [   3    0    0    0    0    2    4 1049    1]\n",
      " [   0    0    0    0    0    2    0    0  121]]\n",
      "\n",
      "Classification Report:\n",
      "                                             precision    recall  f1-score   support\n",
      "\n",
      "              Pepper__bell___Bacterial_spot     0.9758    0.9818    0.9788       329\n",
      "                     Pepper__bell___healthy     0.9877    0.9898    0.9887       488\n",
      "                      Potato___Early_blight     0.9818    0.9788    0.9803       330\n",
      "                       Potato___Late_blight     0.9783    0.9576    0.9678       330\n",
      "                           Potato___healthy     0.8772    1.0000    0.9346        50\n",
      "                           Tomato_Leaf_Mold     0.9533    0.9745    0.9638       314\n",
      "Tomato_Spider_mites_Two_spotted_spider_mite     0.9851    0.9584    0.9716       553\n",
      "      Tomato__Tomato_YellowLeaf__Curl_Virus     0.9906    0.9906    0.9906      1059\n",
      "                Tomato__Tomato_mosaic_virus     0.9380    0.9837    0.9603       123\n",
      "\n",
      "                                   accuracy                         0.9790      3576\n",
      "                                  macro avg     0.9631    0.9795    0.9707      3576\n",
      "                               weighted avg     0.9794    0.9790    0.9791      3576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"Making predictions on the test set...\")\n",
    "# Get predictions (only the classification output)\n",
    "y_pred_probs, _, _ = model.predict(test_ds)\n",
    "y_pred = y_pred_probs.argmax(axis=1)\n",
    "\n",
    "# Extract true labels from the test_ds for evaluation\n",
    "# This requires iterating through the dataset, which might be slow for very large datasets\n",
    "# If test_labels (from train_test_split) is reliable, use that directly.\n",
    "y_true_list = []\n",
    "for images, labels in test_ds:\n",
    "    y_true_list.extend(labels.numpy())\n",
    "y_true = np.array(y_true_list)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8763b00e-8ef9-40d4-944c-59a753a66762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Details:\n",
      "  Index: 284, Name: StatefulPartitionedCall:1, Shape: [1 7 7 1], Dtype: <class 'numpy.float32'>\n",
      "  Index: 278, Name: StatefulPartitionedCall:2, Shape: [   1 1280], Dtype: <class 'numpy.float32'>\n",
      "  Index: 288, Name: StatefulPartitionedCall:0, Shape: [ 1 15], Dtype: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"cbam_attention_tfdata.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Output Details:\")\n",
    "for detail in output_details:\n",
    "    print(f\"  Index: {detail['index']}, Name: {detail['name']}, Shape: {detail['shape']}, Dtype: {detail['dtype']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377a2e76-589d-4ad0-9fcb-3257cadbecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "Index: 0, Shape: [  1 224 224   3], DType: <class 'numpy.float32'>\n",
      "\n",
      "Outputs:\n",
      "Index: 284, Shape: [1 7 7 1], DType: <class 'numpy.float32'>\n",
      "Index: 278, Shape: [   1 1280], DType: <class 'numpy.float32'>\n",
      "Index: 288, Shape: [ 1 15], DType: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"cbam_attention_tfdata.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Inputs:\")\n",
    "for d in input_details:\n",
    "    print(f\"Index: {d['index']}, Shape: {d['shape']}, DType: {d['dtype']}\")\n",
    "\n",
    "print(\"\\nOutputs:\")\n",
    "for d in output_details:\n",
    "    print(f\"Index: {d['index']}, Shape: {d['shape']}, DType: {d['dtype']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b744ca-c25f-4b2f-9360-3d7fee2323bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2600a07-a204-40e3-af0a-dd50c1d37cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
