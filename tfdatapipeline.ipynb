{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fbd44ac-51b5-4d87-9666-e8b76fdf714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import PIL.Image as Image\n",
    "import os,glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling2D, GlobalMaxPooling2D,\n",
    "    Reshape, Dense, Conv2D, Multiply, Add, Activation,\n",
    "    Concatenate, BatchNormalization, ReLU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa5e0935-0154-4efb-965b-20094d201f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('plant-village')\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a64b9cd-6916-4010-b114-f9b0debf2149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting image paths and labels...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e79e01e3ce5443f98609a79b64389ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classes: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2152 images across 3 classes.\n"
     ]
    }
   ],
   "source": [
    "all_image_paths = []\n",
    "all_image_labels = []\n",
    "\n",
    "print(\"Collecting image paths and labels...\")\n",
    "for class_dir in tqdm(base_dir.iterdir(), desc=\"Classes\"):\n",
    "    if class_dir.is_dir():\n",
    "        label = class_dir.name\n",
    "        for img_path in class_dir.glob(\"*.jpg\"):\n",
    "            all_image_paths.append(str(img_path))\n",
    "            all_image_labels.append(label)\n",
    "\n",
    "print(f\"Found {len(all_image_paths)} images across {len(set(all_image_labels))} classes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38e2d0e-96aa-4ffd-8f55-aa12142e9b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label order used by model:\n",
      "0: Potato___Early_blight\n",
      "1: Potato___Late_blight\n",
      "2: Potato___healthy\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "all_image_labels_encoded = le.fit_transform(all_image_labels)\n",
    "\n",
    "print(\"Label order used by model:\")\n",
    "for idx, label in enumerate(le.classes_):\n",
    "    print(f\"{idx}: {label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96201db8-3df2-408a-be44-bce5f1e876c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1441\n",
      "Test samples: 711\n"
     ]
    }
   ],
   "source": [
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    all_image_paths, all_image_labels_encoded, test_size=0.33, random_state=0, stratify=all_image_labels_encoded\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_paths)}\")\n",
    "print(f\"Test samples: {len(test_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9a45272-069e-4575-8c23-0f4f37fa6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3) \n",
    "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    image = image / 255.0  \n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66c5b0b5-5058-44c8-ba13-c39075c18310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, label):\n",
    "    # Apply random horizontal flip\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    # Apply random brightness adjustment\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2) # Adjust brightness by up to +/- 20%\n",
    "\n",
    "    # Apply random contrast adjustment\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2) # Adjust contrast by 80% to 120%\n",
    "\n",
    "    # Optionally, apply random zoom or rotation using Keras preprocessing layers\n",
    "    # (These are typically better applied as part of the Keras model's first layers,\n",
    "    # or you'd need to create them as standalone callables for tf.data.Dataset.map)\n",
    "\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90afc9bd-e189-415f-adc0-f478b28ba5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.data.Dataset for training and testing\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# Apply augmentation ONLY to the training dataset\n",
    "train_ds = train_ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE) # Add this line\n",
    "train_ds = train_ds.shuffle(buffer_size=len(train_paths))\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "test_ds = test_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\n",
    "test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d160c48-cb17-4780-b6b2-ba5406e74f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False # Freeze the base model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b23cd5bf-3436-477a-bdd7-edc947324abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(input_tensor, ratio=8):\n",
    "    \"\"\"Squeeze-and-Excitation block.\"\"\"\n",
    "    channel = input_tensor.shape[-1]\n",
    "    se = GlobalAveragePooling2D()(input_tensor)\n",
    "    se = Reshape((1, 1, channel))(se)\n",
    "    se = Dense(channel // ratio, activation='relu', use_bias=False)(se)\n",
    "    se = Dense(channel, activation='sigmoid', use_bias=False)(se)\n",
    "    return Multiply()([input_tensor, se])  # Squeeze and excite output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4399813-ddb5-415f-a364-6656cd7a90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbam_block_with_outputs(input_feature, ratio=8):\n",
    "    channel = input_feature.shape[-1]\n",
    "\n",
    "    # 1. Apply SE block first (preconditioning) - as in your original code\n",
    "    se_refined = se_block(input_feature, ratio=ratio)\n",
    "\n",
    "    # 2. Channel attention (CBAM)\n",
    "    shared_dense_one = Dense(channel // ratio, activation='relu', name='cbam_dense_1')\n",
    "    shared_dense_two = Dense(channel, name='cbam_dense_2')\n",
    "\n",
    "    avg_pool = GlobalAveragePooling2D()(se_refined)\n",
    "    avg_pool = Reshape((1, 1, channel))(avg_pool)\n",
    "    avg_out = shared_dense_two(shared_dense_one(avg_pool))\n",
    "\n",
    "    max_pool = GlobalMaxPooling2D()(se_refined)\n",
    "    max_pool = Reshape((1, 1, channel))(max_pool)\n",
    "    max_out = shared_dense_two(shared_dense_one(max_pool))\n",
    "\n",
    "    channel_att = Activation('sigmoid')(Add()([avg_out, max_out]))\n",
    "    channel_refined = Multiply()([se_refined, channel_att])  # (H, W, C)\n",
    "\n",
    "    # 3. Spatial attention (CBAM)\n",
    "    avg_sp = tf.reduce_mean(channel_refined, axis=-1, keepdims=True)\n",
    "    max_sp = tf.reduce_max(channel_refined, axis=-1, keepdims=True)\n",
    "    concat = Concatenate()([avg_sp, max_sp])\n",
    "    spatial_att = Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n",
    "\n",
    "    spatial_refined = Multiply()([channel_refined, spatial_att])  # (H, W, C)\n",
    "\n",
    "    return spatial_refined, channel_att, spatial_att\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e860d40f-1afe-4727-a18f-5172257047a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cbam_model(num_classes):\n",
    "    input_tensor = tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "    # Use MobileNetV2 base\n",
    "    x = base_model(input_tensor, training=False)  # Shape: (7, 7, 1280)\n",
    "\n",
    "    # Apply CBAM\n",
    "    channel = x.shape[-1]\n",
    "    cbam_out, channel_att_map, spatial_att_map = cbam_block_with_outputs(x)\n",
    "\n",
    "    # Pool + classify\n",
    "    x = GlobalAveragePooling2D()(cbam_out)  # shape: (None, 1280)\n",
    "    predictions = Dense(num_classes, activation='softmax', name='classification')(x)\n",
    "\n",
    "    # Flatten channel attention for output\n",
    "    # This output is likely for visualization/analysis, not directly for loss calculation\n",
    "    channel_vector = Reshape((channel,))(channel_att_map)  # (None, 1280)\n",
    "    spatial_map = spatial_att_map  # (None, 7, 7, 1)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_tensor, outputs=[predictions, channel_vector, spatial_map])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70ebab08-e725-4684-97e4-22e6e7076b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Epoch 1/10\n",
      "46/46 [==============================] - 116s 2s/step - loss: 0.3693 - classification_loss: 0.3693 - classification_accuracy: 0.8744 - val_loss: 0.1238 - val_classification_loss: 0.1238 - val_classification_accuracy: 0.9550\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 79s 2s/step - loss: 0.0743 - classification_loss: 0.0743 - classification_accuracy: 0.9799 - val_loss: 0.2399 - val_classification_loss: 0.2399 - val_classification_accuracy: 0.9100\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 82s 2s/step - loss: 0.1935 - classification_loss: 0.1935 - classification_accuracy: 0.9417 - val_loss: 0.0924 - val_classification_loss: 0.0924 - val_classification_accuracy: 0.9662\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 79s 2s/step - loss: 0.0483 - classification_loss: 0.0483 - classification_accuracy: 0.9861 - val_loss: 0.0659 - val_classification_loss: 0.0659 - val_classification_accuracy: 0.9705\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 76s 2s/step - loss: 0.0259 - classification_loss: 0.0259 - classification_accuracy: 0.9938 - val_loss: 0.0559 - val_classification_loss: 0.0559 - val_classification_accuracy: 0.9789\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 80s 2s/step - loss: 0.0116 - classification_loss: 0.0116 - classification_accuracy: 0.9972 - val_loss: 0.0690 - val_classification_loss: 0.0690 - val_classification_accuracy: 0.9705\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 90s 2s/step - loss: 0.0152 - classification_loss: 0.0152 - classification_accuracy: 0.9965 - val_loss: 0.0533 - val_classification_loss: 0.0533 - val_classification_accuracy: 0.9789\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 81s 2s/step - loss: 0.0040 - classification_loss: 0.0040 - classification_accuracy: 1.0000 - val_loss: 0.0575 - val_classification_loss: 0.0575 - val_classification_accuracy: 0.9775\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 79s 2s/step - loss: 0.0052 - classification_loss: 0.0052 - classification_accuracy: 0.9986 - val_loss: 0.0628 - val_classification_loss: 0.0628 - val_classification_accuracy: 0.9789\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 76s 2s/step - loss: 0.0035 - classification_loss: 0.0035 - classification_accuracy: 0.9993 - val_loss: 0.1222 - val_classification_loss: 0.1222 - val_classification_accuracy: 0.9648\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(le.classes_)\n",
    "model = build_cbam_model(num_classes=num_classes)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'classification':'sparse_categorical_crossentropy'},  \n",
    "    metrics={'classification':'accuracy'}\n",
    ")\n",
    "\n",
    "epochs = 10 \n",
    "\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_ds\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d3d8e30-3f2e-4cbc-a0d6-4aa1421b7db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_models/cbam_with_attention_outputs_se_tfdata\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_models/cbam_with_attention_outputs_se_tfdata\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kariu\\AppData\\Local\\Temp\\tmpnlfo4d9t\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kariu\\AppData\\Local\\Temp\\tmpnlfo4d9t\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved and converted to TFLite.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"../saved_models/cbam_with_attention_outputs_se_tfdata\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"cbam_attention_tfdata.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model saved and converted to TFLite.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "929ff8f3-ea6a-4378-a579-c642b820fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on the test set...\n",
      "23/23 [==============================] - 25s 1s/step\n",
      "Confusion Matrix:\n",
      "[[329   2   0]\n",
      " [  6 307  17]\n",
      " [  0   0  50]]\n",
      "\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "Potato___Early_blight     0.9821    0.9940    0.9880       331\n",
      " Potato___Late_blight     0.9935    0.9303    0.9609       330\n",
      "     Potato___healthy     0.7463    1.0000    0.8547        50\n",
      "\n",
      "             accuracy                         0.9648       711\n",
      "            macro avg     0.9073    0.9748    0.9345       711\n",
      "         weighted avg     0.9708    0.9648    0.9660       711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"Making predictions on the test set...\")\n",
    "# Get predictions (only the classification output)\n",
    "y_pred_probs, _, _ = model.predict(test_ds)\n",
    "y_pred = y_pred_probs.argmax(axis=1)\n",
    "\n",
    "# Extract true labels from the test_ds for evaluation\n",
    "# This requires iterating through the dataset, which might be slow for very large datasets\n",
    "# If test_labels (from train_test_split) is reliable, use that directly.\n",
    "y_true_list = []\n",
    "for images, labels in test_ds:\n",
    "    y_true_list.extend(labels.numpy())\n",
    "y_true = np.array(y_true_list)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763b00e-8ef9-40d4-944c-59a753a66762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
